{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages including the custom utility module (see utils.py)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main data location\n",
    "data_dir = '/Users/junginpark/data/SEER_1973_2015_TEXTDATA/incidence'\n",
    "\n",
    "# SAS file name\n",
    "sas_filename = os.path.join(data_dir, 'read.seer.research.nov17.sas')\n",
    "\n",
    "# Read SAS file and extract column information.\n",
    "# For each line of the SAS file, it defines how to interpret the TXT files.\n",
    "# For example, below is one of the lines from the SAS file:\n",
    "#\n",
    "# @ 9   REG                  $char10. /* SEER registry */\n",
    "#\n",
    "# This give us the following info:\n",
    "# 1) Starting offset (number of characters from the beginning of a line). In this case, 9.\n",
    "# 2) Width (number of characters). In this case, 10.\n",
    "# 3) Short name (REG).\n",
    "# 4) Long name (SEER registry).\n",
    "#\n",
    "# Below is one of the lines from the TXT files:\n",
    "#\n",
    "# 540000120000001537201 020731932   02022006C53908090380983311        9800...\n",
    "#    from ^     to ^\n",
    "#\n",
    "# Since \"REG\" starts from 9th character and 10 characters long, the REG value of this line is \"0000001537\".\n",
    "\n",
    "# The list \"columns\" below will store the 4 column info (starting offset, width, short name, long name).\n",
    "columns = []\n",
    "\n",
    "# We then read the SAS file.\n",
    "with open(sas_filename, 'r') as f:\n",
    "    # Read all the lines from the file (f), but throw away first 5 lines (not needed).\n",
    "    lines = f.readlines()[5:]\n",
    "    \n",
    "    # For each line..\n",
    "    for line in lines:\n",
    "        # First, clean up the line. Especially the last line of the SAS file contains weird semi-colon\n",
    "        # so we want to clean that up. Remove the left and right white spaces (if exists), \n",
    "        # then remove the semi-colon on the right (if exists), and remove the white spaces again (if exists).\n",
    "        line = line.strip().rstrip(';').strip()\n",
    "        \n",
    "        # Second, \"tokenize\" the line into individual words.\n",
    "        tokens = line.split()\n",
    "        \n",
    "        # For instance, if the line is something like below,\n",
    "        # @ 9   REG                  $char10. /* SEER registry */\n",
    "        # 0-1---2--                  3------- 4- 5--- 6------- 7-\n",
    "        # ^ these are token numbers.\n",
    "        \n",
    "        # tokens[0] is \"@\" so we'll skip it. \n",
    "        # tokens[1] has the start offset. Convert it to integer and subtract 1 because \n",
    "        # Python index starts from 0 where SAS starts from 1.\n",
    "        start_offset = int(tokens[1]) - 1 \n",
    "\n",
    "        # tokens[2] has the short name. It is string already so just copy it.\n",
    "        short_name = tokens[2]\n",
    "        \n",
    "        # tokens[3] has the width in a special format (ex. $char10) and we want to convert it into integer (10).\n",
    "        # So from tokens[3], take sub-string from 5th chracter up to the last-1 character using slice operator ([5:-1]).\n",
    "        width = int(tokens[3][5:-1])\n",
    "        \n",
    "        # Read the long name. Join tokens[5], tokens[6], ... tokens[-1] with a space.\n",
    "        # So ['SEER', 'registry'] becomes 'SEER registry'.\n",
    "        long_name = ' '.join(tokens[5:-1])\n",
    "        \n",
    "        # Store this column info\n",
    "        columns.append((start_offset, width, short_name, long_name))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/junginpark/data/SEER_1973_2015_TEXTDATA/incidence/yr1973_2015.seer9/BREAST.TXT',\n",
       " '/Users/junginpark/data/SEER_1973_2015_TEXTDATA/incidence/yr1973_2015.seer9/COLRECT.TXT',\n",
       " '/Users/junginpark/data/SEER_1973_2015_TEXTDATA/incidence/yr1973_2015.seer9/DIGOTHR.TXT',\n",
       " '/Users/junginpark/data/SEER_1973_2015_TEXTDATA/incidence/yr1973_2015.seer9/FEMGEN.TXT',\n",
       " '/Users/junginpark/data/SEER_1973_2015_TEXTDATA/incidence/yr1973_2015.seer9/LYMYLEUK.TXT']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all the TXT file names in \"data_dir\" directory.\n",
    "# We are using our own utility function \"utils.get_all_files(...)\" for that.\n",
    "# We are also sorting the file names, just to make it easier to debug.\n",
    "txt_filenames = sorted(utils.get_all_files(data_dir, '*.TXT'))\n",
    "\n",
    "# Take a look at first 5 file names.\n",
    "txt_filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each file name, convert it into a Pandas DataFrame.\n",
    "for txt_filename in txt_filenames:\n",
    "    print(f'Processing {txt_filename}...')\n",
    "    \n",
    "    # This will store the list of all the rows.\n",
    "    data = []\n",
    "    \n",
    "    # Open up the TXT file.\n",
    "    with open(txt_filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        # Parse out the i-th line.\n",
    "        for i, line in enumerate(lines):\n",
    "            # When creating our own DataFrame, each row has to be a dictionary\n",
    "            # where its keys are the name of the columns.\n",
    "            row = {}\n",
    "            \n",
    "            # List \"columns\" has the columns information from SAS.\n",
    "            # We will load each column info and parse the row.\n",
    "            for col in columns:\n",
    "                # Expand this column info into individual variables.\n",
    "                start_offset, width, short_name, _ = col\n",
    "                # We will use \"short_name\" as a key/column name (and not use long_name).\n",
    "                # Read the line from start_offset to start_offset + width.\n",
    "                row[short_name] = line[start_offset:start_offset + width]\n",
    "                \n",
    "            # Append the row to the data list\n",
    "            data.append(row)\n",
    "            \n",
    "            # Print this message for every 100000 rows.\n",
    "            if (i + 1) % 100000 == 0:\n",
    "                print(f'> Read {i+1} rows...')\n",
    "        print(f'> Read {i+1} rows...')\n",
    "        \n",
    "    # Convert data (list of dictionaries) into DataFrame.\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Define the directory to put new CSV files.\n",
    "    csv_dir = os.path.join(data_dir, 'csv')\n",
    "    \n",
    "    # Define the CSV filename.\n",
    "    csv_filename = txt_filename.replace(data_dir, csv_dir).replace('.TXT', '.csv')\n",
    "    \n",
    "    # Create CSV directory if it doesn't exist.\n",
    "    if not os.path.exists(os.path.dirname(csv_filename)):\n",
    "        os.makedirs(os.path.dirname(csv_filename))\n",
    "    \n",
    "    print(f'> Writing to {csv_filename}...')\n",
    "    df.to_csv(csv_filename)\n",
    "    \n",
    "    print(f'> Done processing {txt_filename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
